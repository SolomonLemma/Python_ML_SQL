{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Regression with Tensorflow & Keras\n",
    "\n",
    "**OBJECTIVES**\n",
    "\n",
    "- Build regression models using `tensorflow` & `keras`\n",
    "- Refine models by adjusting the architecture of a network\n",
    "- Use regularization to attempt performance improvement\n",
    "- Save and reuse the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali = fetch_california_housing()\n",
    "X, y = cali.data, cali.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "Load the california housing data and create a dataframe called `cali_df` below.  Make sure to add the target feature and name this `price`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(cali.data, columns=cali.feature_names)\n",
    "y = pd.DataFrame(cali.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(cali.data, columns=cali.feature_names)\n",
    "y = pd.DataFrame(cali.target)\n",
    "y.columns = ['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's keep everything in one df for easy access\n",
    "cali_df = pd.concat([X, y], axis=1)\n",
    "cali_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  price  \n",
       "0    -122.23  4.526  \n",
       "1    -122.22  3.585  \n",
       "2    -122.24  3.521  \n",
       "3    -122.25  3.413  \n",
       "4    -122.25  3.422  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cali_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Create a train/test split using some of the features in your X and setting y to be the `price` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cali_df.drop(['price'], axis=1)\n",
    "y = cali_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Setup a `Sequential` model with one layer containing 24 nodes.  Make sure to include the output layer and use a relu activation for the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, \n",
    "                input_shape=(8,),\n",
    "                activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - continued\n",
    "\n",
    "Set up the compilation of the network.  Use an adam optimizer and appropriate loss function with the mean squared error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "Fit the model on the training data with 100 epochs (and sequester the output with `verbose = 0`). Save the fit model to the variable `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_sc, y_train, validation_data=(X_test_sc, y_test), \n",
    "          epochs=100, batch_size=512, verbose=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "Use `matplotlib` to plot the training loss and validation loss, and the training mean squared error alongside the validation data.  Side by side subplots please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjS0lEQVR4nO3deZSU9Z3v8fe3lq5umqXZFGSxSa4ryGaLqDFBTXAdnUziGZ0kiobLSDQxmTBucxL15uTcOTleNcZEDpNojJO43LiMk+BNXK84uSoNwQVBcI0tRLCRZu2lqr73j+epprrppqubaoqn+vM6p07Vs9RTvx/L5/d7fs9m7o6IiERfrNQFEBGR4lCgi4iUCQW6iEiZUKCLiJQJBbqISJlIlOqHR40a5bW1taX6eRGRSFqxYsXH7j66q2UlC/Ta2lrq6+tL9fMiIpFkZu93t0xDLiIiZUKBLiJSJhToIiJlouAxdDOLA/XAh+5+XqdlBvwYOAfYBcxz95XFLKiI7L+2tjYaGhpobm4udVGkB5WVlYwfP55kMlnwd3pzUPRqYA0wtItlZwNHhK8TgbvCdxE5iDQ0NDBkyBBqa2sJ+mFyMHJ3GhsbaWhoYNKkSQV/r6AhFzMbD5wL/LybVS4AfuWBF4EaMxtbcClE5IBobm5m5MiRCvODnJkxcuTIXu9JFTqGfjtwDZDtZvk44IO86YZwXgdmtsDM6s2sfvPmzb0pp4gUicI8Gvry99RjoJvZecAmd1+xr9W6mLfXfXndfYm717l73ejRXZ4X36M3/7qd//XHN2nc0dKn74uIlKtCeuinAOeb2XvAA8DpZvbvndZpACbkTY8HNhSlhJ28vXkHP3nmLT7e0dofmxeRftTY2Mj06dOZPn06Y8aMYdy4ce3Tra37/j9dX1/Pt771rV79Xm1tLR9//PH+FDlSejwo6u7XA9cDmNkcYJG7f7XTao8DV5nZAwQHQ5vcfWNxixqoiAdtUEs60x+bF5F+NHLkSFatWgXATTfdxODBg1m0aFH78nQ6TSLRdSzV1dVRV1d3IIoZWX0+D93MrjCzK8LJpcA7wFvAvwHfKELZupRK5gK9u+F8EYmSefPm8U//9E+cdtppXHvttbz88sucfPLJzJgxg5NPPpk333wTgOeee47zzgvOmL7pppu4/PLLmTNnDp/61Ke44447evydW2+9lSlTpjBlyhRuv/12AHbu3Mm5557LtGnTmDJlCg8++CAA1113HcceeyxTp07t0OAc7Hp1Lxd3fw54Lvy8OG++A1cWs2DdyfXQWxXoIvvl5v9czRsbthV1m8ceNpQb/2Zyr7+3bt06nnrqKeLxONu2beP5558nkUjw1FNPccMNN/Dwww/v9Z21a9fy7LPPsn37do466igWLlzY7TnbK1as4J577uGll17C3TnxxBP53Oc+xzvvvMNhhx3G73//ewCamprYsmULjz76KGvXrsXM2Lp1a6/rUyqRu1I0lYwDGnIRKScXXngh8Xjwf7upqYkLL7yQKVOm8J3vfIfVq1d3+Z1zzz2XVCrFqFGjOOSQQ/joo4+63f4LL7zAF7/4Raqrqxk8eDB/93d/x7JlyzjuuON46qmnuPbaa1m2bBnDhg1j6NChVFZWMn/+fB555BEGDRrUL3XuDyW722JfqYcuUhx96Un3l+rq6vbP3/ve9zjttNN49NFHee+995gzZ06X30mlUu2f4/E46XS62+0Hgwh7O/LII1mxYgVLly7l+uuvZ+7cuXz/+9/n5Zdf5umnn+aBBx7gzjvv5JlnnulbxQ6wCPbQNYYuUs6ampoYNy64jOWXv/xlUbb52c9+lscee4xdu3axc+dOHn30UU499VQ2bNjAoEGD+OpXv8qiRYtYuXIlO3bsoKmpiXPOOYfbb7+9/SBuFESuh55KKNBFytk111zDpZdeyq233srpp59elG3OnDmTefPmMWvWLADmz5/PjBkz+MMf/sA///M/E4vFSCaT3HXXXWzfvp0LLriA5uZm3J3bbrutKGU4EKy7XZH+VldX5315wMWm7c3M+uHT/OBvp/C12Yf3Q8lEyteaNWs45phjSl0MKVBXf19mtsLduzx/M3pDLongwInG0EVEOopgoOvCIhGRrkQu0NuvFG1TD11EJF/kAj0WM5JxozWjQBcRyRe5QIdgHF09dBGRjiIZ6BWJGK0ZjaGLiOSLZKCnEjH10EUi6EDfPnegidyFRRAEusbQRaKnXG+fu69yH0iR7KFXqIcuUjb68/a5CxcupK6ujsmTJ3PjjTe2z1++fDknn3wy06ZNY9asWWzfvp1MJsOiRYs47rjjmDp1Kj/5yU+Ajg/JqK+vb7+3zE033cSCBQuYO3cul1xyCe+99x6nnnoqM2fOZObMmfzpT39q/70f/ehHHHfccUybNo3rrruOt99+m5kzZ7YvX79+Pccff/x+/1mWvknpg1Qirh66yP564jr462vF3eaY4+Dsf+311/rr9rk//OEPGTFiBJlMhjPOOINXX32Vo48+mr//+7/nwQcf5IQTTmDbtm1UVVWxZMkS3n33Xf785z+TSCTYsmVLj+VesWIFL7zwAlVVVezatYsnn3ySyspK1q9fz8UXX0x9fT1PPPEEjz32GC+99BKDBg1iy5YtjBgxgmHDhrFq1SqmT5/OPffcw7x583r959ZZJAO9IhHThUUiZaTz7XMvvfRS1q9fj5nR1tbW5Xdyt89NpVLtt88dP358h3UeeughlixZQjqdZuPGjbzxxhuYGWPHjuWEE04AYOjQoQA89dRTXHHFFe1DJyNGjOix3Oeffz5VVVUAtLW1cdVVV7Fq1Sri8Tjr1q1r3+5ll13Wfhve3Hbnz5/PPffcw6233sqDDz7Iyy+/3Ks/s670GOhmVgk8D6TC9X/r7jd2WmcO8B/Au+GsR9z9f+x36bqhg6IiRdCHnnR/6Y/b57777rvccsstLF++nOHDhzNv3rz2G26Z7f1c++7mJxIJstkgb5qbm7st92233cahhx7KK6+8QjabpbKycp/b/dKXvsTNN9/M6aefzvHHH8/IkSO7rGdvFDKG3gKc7u7TgOnAWWY2u4v1lrn79PDVb2EOudMWFegi5ahYt8/dtm0b1dXVDBs2jI8++ognnngCgKOPPpoNGzawfPlyALZv3046nWbu3LksXry4vWHIDbnU1tayYsUKgC6HfvLLPXbsWGKxGPfddx+Z8NTquXPncvfdd7Nr164O262srOTMM89k4cKFXHbZZX2uZ74eA90DO8LJZPgqzS0aQ+qhi5Sva665huuvv55TTjmlPRT7Ytq0acyYMYPJkydz+eWXc8oppwBQUVHBgw8+yDe/+U2mTZvGF77wBZqbm5k/fz4TJ05k6tSpTJs2jd/85jcA3HjjjVx99dWceuqp7cNCXfnGN77Bvffey+zZs1m3bl177/2ss87i/PPPp66ujunTp3PLLbe0f+crX/kKZsbcuXP7XM98Bd0+18ziwArgvwE/dfdrOy2fAzwMNAAbgEXuvtdzo8xsAbAAYOLEice///77fSr0N+//M69/2MSzi+b06fsiA5Vun3twueWWW2hqauIHP/hBl8t7e/vcgg6KunsGmG5mNcCjZjbF3V/PW2UlcLi77zCzc4DHgCO62M4SYAkE90Mv5Le7EvTQdVBURKLri1/8Im+//XZRH2/Xq7Nc3H2rmT0HnAW8njd/W97npWb2MzMb5e4fF62keXRhkYhE3aOPPlr0bfY4hm5mo8OeOWZWBXweWNtpnTEWHsY1s1nhdhuLXtqQLiwS6btSPaVMeqcvf0+F9NDHAveG4+gx4CF3/52ZXRH+6GLgy8BCM0sDu4GLvB//1aQScVrUQxfptcrKShobGxk5cmSXp9LJwcHdaWxsbD/1sVA9Brq7vwrM6GL+4rzPdwJ39uqX90NFIkZrOtvt+Z0i0rXx48fT0NDA5s2bS10U6UFlZeVeF0r1JJJXiu55DF2WymT3pxGJSEfJZJJJkyaVuhjSTyJ5c65coOvAqIjIHpEOdB0YFRHZI5KBXqEeuojIXiIZ6KlEMG6ui4tERPaIZKCrhy4isrdIBrrG0EVE9hbRQA+GXNRDFxHZI5KBXqEeuojIXiIZ6HsuLNJBURGRnEgGevtB0bR66CIiOZEM9PxL/0VEJBDJQFcPXURkb5EM9PYLizSGLiLSLpKBXqEhFxGRvUQy0DWGLiKyt0IeQVdpZi+b2StmttrMbu5iHTOzO8zsLTN71cxm9k9xAymNoYuI7KWQB1y0AKe7+w4zSwIvmNkT7v5i3jpnA0eErxOBu8L3fmFmVMRj6qGLiOTpsYfugR3hZDJ8dX5e6AXAr8J1XwRqzGxscYvaUSoR00FREZE8BY2hm1nczFYBm4An3f2lTquMAz7Im24I53XezgIzqzez+v19pmHuuaIiIhIoKNDdPePu04HxwCwzm9Jpla6e1Ny5F4+7L3H3OnevGz16dK8Lmy/ooSvQRURyenWWi7tvBZ4Dzuq0qAGYkDc9HtiwPwXriXroIiIdFXKWy2gzqwk/VwGfB9Z2Wu1x4JLwbJfZQJO7byx2YfOlEnGNoYuI5CnkLJexwL1mFidoAB5y99+Z2RUA7r4YWAqcA7wF7AIu66fytlMPXUSkox4D3d1fBWZ0MX9x3mcHrixu0fZNY+giIh1F8kpRgFRSPXQRkXyRDXRdWCQi0lFkA10HRUVEOopsoOugqIhIR5ENdB0UFRHpKLKBrh66iEhHkQ30YAxdgS4ikhPZQFcPXUSko8gGeioRozWTJZvd6x5gIiIDUmQDPfdc0daMeukiIhDhQNdzRUVEOopuoCfjALq4SEQkFN1Aj+tB0SIi+aIb6EkNuYiI5ItsoFeohy4i0kFkA109dBGRjgp5BN0EM3vWzNaY2Wozu7qLdeaYWZOZrQpf3++f4u5REQ8OiqqHLiISKOQRdGngu+6+0syGACvM7El3f6PTesvc/bziF7Fre3roOstFRAQK6KG7+0Z3Xxl+3g6sAcb1d8F6ojF0EZGOejWGbma1BM8XfamLxSeZ2Stm9oSZTe7m+wvMrN7M6jdv3tz70ubRGLqISEcFB7qZDQYeBr7t7ts6LV4JHO7u04CfAI91tQ13X+Lude5eN3r06D4WOZBK6MIiEZF8BQW6mSUJwvzX7v5I5+Xuvs3dd4SflwJJMxtV1JJ20n4vF/XQRUSAws5yMeAXwBp3v7WbdcaE62Fms8LtNhazoJ3pXi4iIh0VcpbLKcDXgNfMbFU47wZgIoC7Lwa+DCw0szSwG7jI3fv1vrbqoYuIdNRjoLv7C4D1sM6dwJ3FKlQh1EMXEekosleK5k5bVKCLiAQiG+hmRkUiprNcRERCkQ10CG6hqzF0EZFAtAM9GdOQi4hIKNqBnojT0qZAFxGBiAd6RSKmh0SLiIQiHeipRIyWNh0UFRGBiAe6eugiIntEOtCDHroCXUQEIh7o6qGLiOwR6UBPJeK6sEhEJBTpQK/QhUUiIu0iHei6sEhEZI9oB7oOioqItIt0oOugqIjIHpEO9ODSfx0UFRGBwh5BN8HMnjWzNWa22syu7mIdM7M7zOwtM3vVzGb2T3E7Ug9dRGSPQh5Blwa+6+4rzWwIsMLMnnT3N/LWORs4InydCNwVvverVCJGW8bJZp1YbJ8PVRIRKXs99tDdfaO7rww/bwfWAOM6rXYB8CsPvAjUmNnYope2k/bniqqXLiLSuzF0M6sFZgAvdVo0Dvggb7qBvUMfM1tgZvVmVr958+ZeFnVvqUQcQGe6iIjQi0A3s8HAw8C33X1b58VdfMX3muG+xN3r3L1u9OjRvStpF3I99JaMDoyKiBQU6GaWJAjzX7v7I12s0gBMyJseD2zY/+LtWyoX6Oqhi4gUdJaLAb8A1rj7rd2s9jhwSXi2y2ygyd03FrGcXWoPdF0tKiJS0FkupwBfA14zs1XhvBuAiQDuvhhYCpwDvAXsAi4rekm7kAt03c9FRKSAQHf3F+h6jDx/HQeuLFahCtV+UFR3XBQRifiVosmg+Lt1taiISLQDvaaqAoCtu9pKXBIRkdKLdKCPHBwE+padrSUuiYhI6UU60IcPUqCLiOREOtArEjGGVCYU6CIiRDzQAUZUV9CoQBcRKY9A/0SBLiIS/UAfqR66iAhQBoE+fFAFW3a2lLoYIiIlF/lAHzG4gk92thFcrCoiMnBFPtBHVlfQmsmyoyVd6qKIiJRU5ANd56KLiAQiH+i6WlREJBD5QB9RnQIU6CIi0Q/0cMhFpy6KyEAX/UAPh1x0cZGIDHSFPILubjPbZGavd7N8jpk1mdmq8PX94heze9UVcSoSMQ25iMiAV8gj6H4J3An8ah/rLHP384pSol4yM0YM0tWiIiI99tDd/XlgywEoS5/pfi4iIsUbQz/JzF4xsyfMbHJ3K5nZAjOrN7P6zZs3F+mng1MX1UMXkYGuGIG+Ejjc3acBPwEe625Fd1/i7nXuXjd69Ogi/HRgRHWFxtBFZMDb70B3923uviP8vBRImtmo/S5ZLwwfpCEXEZH9DnQzG2NmFn6eFW6zcX+32xsjqyvY3pKmJZ05kD8rInJQ6fEsFzO7H5gDjDKzBuBGIAng7ouBLwMLzSwN7AYu8gN868M956K3MWZY/ED+tIjIQaPHQHf3i3tYfifBaY0lMyLvBl1jhlWWsigiIiUT+StFITgoCrqfi4gMbGUR6Lk7LjbqyUUiMoCVRaDn7omuM11EZCAri0CvGVSBmYZcRGRgK4tAj8eM4bqfi4gMcGUR6ADDByX5ZJcCXUQGrrIJ9JHVKRp3KNBFZOAqm0DX/VxEZKArn0AfXKEhFxEZ0Mon0AdV8MmuNrLZA3rXARGRg0b5BHp1BZms07S7rdRFEREpibIJ9NzVols07CIiA1TZBPrwQbqfi4gMbGUT6KOHpAD48JPdJS6JiEhplE2gH3noEIYPSvJ/1xXvWaUiIlFSNoEejxmnHXUIz765iYzOdBGRAajHQDezu81sk5m93s1yM7M7zOwtM3vVzGYWv5iFOf2YQ9i6q42Vf/mkVEUQESmZQnrovwTO2sfys4EjwtcC4K79L1bffPbI0SRixtNrNpWqCCIiJdNjoLv788CWfaxyAfArD7wI1JjZ2GIVsDeGViaZNWkET6/5qBQ/LyJSUsUYQx8HfJA33RDO24uZLTCzejOr37y5fw5ennHMoazftIO/NO7ql+2LiBysihHo1sW8Lo9KuvsSd69z97rRo0cX4af3dsbRhwDw9Fr10kVkYClGoDcAE/KmxwMbirDdPqkdVc2nR1drHF1EBpxiBPrjwCXh2S6zgSZ331iE7fbZGcccykvvNrK9Wfd1EZGBo5DTFu8H/h9wlJk1mNnXzewKM7siXGUp8A7wFvBvwDf6rbQFOuPoQ2jLOP/5SknbFRGRAyrR0wrufnEPyx24smglKoK62hGcUDucH/zuDWYeXsPRY4aWukgiIv2ubK4UzRePGT/9h5kMrkxwxX0rdEtdERkQyjLQAQ4ZWsnPvjKThk92892HVunBFyJS9so20AFOqB3Bv5x7DE+t2cSNj6/WPV5EpKz1OIYedfNOrmVjUzNLnn+Hj7Y1c8fFM6hMxktdLBGRoivrHjqAmXHDOcdw498cy5NrPuLif3uRxh0tpS6WiEjRlX2g51x2yiTu+spM3tiwjfPv/C9Wb2gqdZFERIpqwAQ6wFlTxvLQP55E1p0v3fUn/mPVh6UukohI0QyoQAeYNqGGx6/6DFPH1XD1A6u4+T9X05LOlLpYIiL7bcAFOgTPH/33+Scy7+Ra7vmv9/jSXX/i3Y93lrpYIiL7ZUAGOkBFIsZN509mydeOp+GT3Zx3xzJ+u6KB4MJXEZHoGbCBnjN38hiWfutUJh82jEX/+xX+8b4VfKyzYEQkggZ8oAMcVlPF/Qtmc/3ZR/Pcm5uZe9vz/P7Vjeqti0ikKNBD8Zjxj5/7NL/71mc4rKaSK3+zknn3LOf9Ro2ti0g0KNA7OfLQITz2jVP43nnHUv/eFr5w2/Pc+uQ6drSkS100EZF9UqB3IRGP8fXPTOLp785h7rGHcsfT6/nsj57l58veoblNpziKyMHJSjVOXFdX5/X19SX57d565YOt3PLHN1m2/mNGD0lx6UmH85UTD2d4dUWpiyYiA4yZrXD3uq6WFdRDN7OzzOxNM3vLzK7rYvkcM2sys1Xh6/v7W+iDybQJNdz39RO5/7/P5pixQ7nlj+s46V+f5vpHXtMtBETkoNHj3RbNLA78FPgCwQOhl5vZ4+7+RqdVl7n7ef1QxoPGSZ8eyUmfHsm6j7bzi2Xv8sjKBu5/+S9Mm1DDP8yawFmTxzJsULLUxRSRAarHIRczOwm4yd3PDKevB3D3/5m3zhxgUW8CPUpDLt1p2tXGwysb+PVL7/P25p0k48apR4zmnOPGcvrRhzBCQzIiUmT7GnIp5H7o44AP8qYbgBO7WO8kM3sF2EAQ7qu7KMgCYAHAxIkTC/jpg9uwQUku/8wkLjulllUfbGXpaxtZ+tpfeWbtJsxg+oQaTj/qEE769EiOGz+MVEL3YReR/lNID/1C4Ex3nx9Ofw2Y5e7fzFtnKJB19x1mdg7wY3c/Yl/bLYceelfcndc+bOKZtZt4du0mXmkIxthTiRjTJ9QwY+Jwpo0fxtQJNRw2rBIzK3GJRSRK9reH3gBMyJseT9ALb+fu2/I+LzWzn5nZKHf/uC8FjjIzY+r4GqaOr+Hbnz+SLTtbWf7eFl5+dwvL39vCL154h7ZM0IgOqUxw1KFDOOLQIUwaNYiJI6qZOGIQ40dUMbRSY/Ei0juFBPpy4AgzmwR8CFwE/EP+CmY2BvjI3d3MZhGcPdNY7MJG0YjqCs6cPIYzJ48BoCWdYc3G7bzasJU3/7qd9R/t4InXN7J1V1uH7w2pTDCuporDaqo4dGglY4ZWMnpIihHVFYwcXBG8V1cwtDJJLKZevogUEOjunjazq4A/AHHgbndfbWZXhMsXA18GFppZGtgNXOS6EUqXUok40yfUMH1CTYf5Tbva+MuWXby/ZScffrKbD7fu5sNPdvPXbc288sFWGne2drm9eMwYVpVkaGWCIZVJhlYlqKmqYNigJMOqkgypTDC0MngfUplgcCpJdSrO4FSC6lSCwakEqURMQz8iZUAXFkVESzrDlp2tNO5oZcvO4NW4s5UtO1vYuquN7c1ptje30bQ7eG3dFbynsz3//ZpBVTJOVTJOZTLOoIo4VRXBdFVFMF0ZLs/Ny02nkjFSiTgViRip8FWZ7DidSsTb3ysSMSoSMeLaqxDpk/0dQ5eDQCoRZ+ywKsYOqyr4O+5Oc1uW7c1tbGtOs6MlzY7mNDta2tjRkmFnSzCvpS3D7rYMu1qD992tez437mjlg9Y0zW1ZmsP1mtsyFNBO7FPMgjqlkjEq4rH2hiEVBn5FPHhPxmMkYkYyHiMZN1KJOJXJoNHIrZubl9te/ndiMSMRM2JmVCZjDE4lGJRKUJn7nfC3tIci5UCBXsbMLOhpV8Q5ZGjxtuvutGay7G7N0JLO0tKWpSUdfk5naWnL0JIJ5rdmgunWTG69LK3pLG2Z4Dut4Xeaw3Vy063pLDtb0rRmsqQzTlsmS1vGaUln2huX1kyWYu1gxiwYvkrEgoajIhGnIm57Qj8Ra1+WiAV7GLnGIhEzkmHDkIwbiXiMZLg8ZkbMggeqVIYNTjwWI26E38/9XrD9RNzyGrC992biZu17P7kGL9coxXONV8xwd9yDvS81VgOHAl16zczC3nRpz6t39/aQzzUKzW1Z0tk9jUDWnXTGybjT0pZlR0uanS3p9kajJZ2hLePBelknHTYcucal/ZUJGqF0xklns7SknYxDJvyt/OVt4XcdcIdMNlie2d/dmj6qSMSoCofB3IM/N4e8PaPgDiBZh6x7OD/YAzIgV+pcQxM0PkFjZQYxs6CBMyMeY68GzYB43MIGL3jlGlALG7yYBesnY2GDGLegrOGv5/beKpPx9t80gkbRCP5N5hq0XFly4jEjldvbi+9pZA2DcDUz2usUjxnxsDxRo0CXyDIzKhJBj3VIqQtTgLZM0Ohks0FwZtyDsA8bjEzWOzYKuYbBO26jQ2OTye3xeIfGKwi5IKRb0pn2vahcgAK0pZ3mcFkuJDFoS2dpDve0nPbMa99rakkHDWXWIZv19rpks0Hjlc5mSWedTCaI41xj2VbEPaoDIb/hyE3n70XlN0h71gr29mJ5DUt7o2d79uq+fPx4Lj25tuhlVqCLHCC53ulA5WEDlmt8smEj4ATzM2GDlM44bdlse88b9jRKueM3jod7G+zZljuZLKQz2fa9CndIZ8O9rnSWTDbbvieSv8OUK1s662SzuQYqeM/JOu17cJmsd2jU2rfDnnmZcNhrT9mC8mXdqaron71bBbqIHBBmFvRudQeMfjNwuwsiImVGgS4iUiYU6CIiZUKBLiJSJhToIiJlQoEuIlImFOgiImVCgS4iUiYU6CIiZUKBLiJSJgoKdDM7y8zeNLO3zOy6Lpabmd0RLn/VzGYWv6giIrIvPQa6mcWBnwJnA8cCF5vZsZ1WOxs4InwtAO4qcjlFRKQHhdycaxbwlru/A2BmDwAXAG/krXMB8KvwOaIvmlmNmY11941FL/H6J+EPN0C6GdKtkGnZs2xf9+Y82G7yH6X7iIpIcc1eCHP2GuzYb4UE+jjgg7zpBuDEAtYZB3QIdDNbQNCDZ+LEib0ta6CyBg45FhKVkEhBvKJTWHcV3EUMz9xjYIriIGtkRKQP8u8aX6AxU/ulJIUEeiEJWVCKuvsSYAkED4ku4Lf3NuEEmHBvn74qIlLOCjko2gBMyJseD2zowzoiItKPCgn05cARZjbJzCqAi4DHO63zOHBJeLbLbKCpX8bPRUSkWz0Oubh72syuAv4AxIG73X21mV0RLl8MLAXOAd4CdgGX9V+RRUSkKwU9gs7dlxKEdv68xXmfHbiyuEUTEZHe0JWiIiJlQoEuIlImFOgiImVCgS4iUibMS3QJupltBt7v49dHAR8XsThRMRDrPRDrDAOz3gOxztD7eh/u7qO7WlCyQN8fZlbv7nWlLseBNhDrPRDrDAOz3gOxzlDcemvIRUSkTCjQRUTKRFQDfUmpC1AiA7HeA7HOMDDrPRDrDEWsdyTH0EVEZG9R7aGLiEgnCnQRkTIRuUDv6YHV5cDMJpjZs2a2xsxWm9nV4fwRZvakma0P34eXuqzFZmZxM/uzmf0unB4Ida4xs9+a2drw7/ykAVLv74T/vl83s/vNrLLc6m1md5vZJjN7PW9et3U0s+vDbHvTzM7s7e9FKtALfGB1OUgD33X3Y4DZwJVhPa8Dnnb3I4Cnw+lyczWwJm96INT5x8D/cfejgWkE9S/repvZOOBbQJ27TyG4NfdFlF+9fwmc1Wlel3UM/49fBEwOv/OzMPMKFqlAJ++B1e7eCuQeWF1W3H2ju68MP28n+A8+jqCuuefv3Qv8bUkK2E/MbDxwLvDzvNnlXuehwGeBXwC4e6u7b6XM6x1KAFVmlgAGETzlrKzq7e7PA1s6ze6ujhcAD7h7i7u/S/B8iVm9+b2oBXp3D6MuW2ZWC8wAXgIOzT0JKnw/pIRF6w+3A9cA2bx55V7nTwGbgXvCoaafm1k1ZV5vd/8QuAX4C8HD5Jvc/Y+Ueb1D3dVxv/MtaoFe0MOoy4WZDQYeBr7t7ttKXZ7+ZGbnAZvcfUWpy3KAJYCZwF3uPgPYSfSHGXoUjhtfAEwCDgOqzeyrpS1Vye13vkUt0AfMw6jNLEkQ5r9290fC2R+Z2dhw+VhgU6nK1w9OAc43s/cIhtJON7N/p7zrDMG/6QZ3fymc/i1BwJd7vT8PvOvum929DXgEOJnyrzd0X8f9zreoBXohD6yOPDMzgjHVNe5+a96ix4FLw8+XAv9xoMvWX9z9encf7+61BH+vz7j7VynjOgO4+1+BD8zsqHDWGcAblHm9CYZaZpvZoPDf+xkEx4rKvd7QfR0fBy4ys5SZTQKOAF7u1ZbdPVIvgodRrwPeBv6l1OXppzp+hmBX61VgVfg6BxhJcFR8ffg+otRl7af6zwF+F34u+zoD04H68O/7MWD4AKn3zcBa4HXgPiBVbvUG7ic4RtBG0AP/+r7qCPxLmG1vAmf39vd06b+ISJmI2pCLiIh0Q4EuIlImFOgiImVCgS4iUiYU6CIiZUKBLiJSJhToIiJl4v8DerdSQpTFG/gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "\n",
    "Let's make a second network that is a bit deeper and more complex. Also, let's now use all the features and see if we help the model.  Use 3 layers, with 64, 128, and 64 nodes respectively in the hidden layers and a `relu` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, \n",
    "                input_shape=(8,),\n",
    "                activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7\n",
    "\n",
    "Add a `BatchNormalization` layer prior to our first dense layer in the network above and repeat the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, \n",
    "                input_shape=(8,),\n",
    "                activation='relu'))\n",
    "BatchNormalization(),\n",
    "model.add(Dense(128, activation='relu'))\n",
    "BatchNormalization(),\n",
    "model.add(Dense(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_sc, y_train, validation_data=(X_test_sc, y_test), \n",
    "          epochs=100, batch_size=512, verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Does this change anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Yes it has change. As shown in the figure the train lose higher with the values lower than about 15. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8\n",
    "\n",
    "Early Stopping.  It seems that we may not need all 100 epochs to train the data.  Include an `EarlyStopping` callback in your model from above.  Set the `patience` equal to 5.  How many epochs do you think are appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "484/484 [==============================] - 0s 698us/step - loss: 307.9182 - accuracy: 0.0000e+00 - val_loss: 4.1016 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "484/484 [==============================] - 0s 588us/step - loss: 4.1370 - accuracy: 2.5840e-04 - val_loss: 1.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "484/484 [==============================] - 0s 620us/step - loss: 2.2794 - accuracy: 1.2920e-04 - val_loss: 1.4628 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "484/484 [==============================] - 0s 620us/step - loss: 2.9399 - accuracy: 1.9380e-04 - val_loss: 1.8478 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "484/484 [==============================] - 0s 588us/step - loss: 3.0762 - accuracy: 1.9380e-04 - val_loss: 5.0029 - val_accuracy: 3.8760e-04\n",
      "Epoch 6/100\n",
      "484/484 [==============================] - 0s 620us/step - loss: 10.0684 - accuracy: 1.9380e-04 - val_loss: 2.4464 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "484/484 [==============================] - 0s 621us/step - loss: 1.5951 - accuracy: 1.2920e-04 - val_loss: 0.9512 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "484/484 [==============================] - 0s 575us/step - loss: 1.1596 - accuracy: 1.2920e-04 - val_loss: 0.9338 - val_accuracy: 1.9380e-04\n",
      "Epoch 9/100\n",
      "484/484 [==============================] - 0s 620us/step - loss: 1.5402 - accuracy: 1.2920e-04 - val_loss: 0.8680 - val_accuracy: 5.8140e-04\n",
      "Epoch 10/100\n",
      "484/484 [==============================] - 0s 589us/step - loss: 5.3773 - accuracy: 3.2300e-04 - val_loss: 0.8123 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "484/484 [==============================] - 0s 588us/step - loss: 2.6729 - accuracy: 1.2920e-04 - val_loss: 0.8202 - val_accuracy: 9.6899e-04\n",
      "Epoch 12/100\n",
      "484/484 [==============================] - 0s 620us/step - loss: 1.0212 - accuracy: 6.4599e-05 - val_loss: 0.7537 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "484/484 [==============================] - 0s 588us/step - loss: 1.2493 - accuracy: 1.9380e-04 - val_loss: 0.6983 - val_accuracy: 7.7519e-04\n",
      "Epoch 14/100\n",
      "484/484 [==============================] - 0s 599us/step - loss: 3.6269 - accuracy: 0.0000e+00 - val_loss: 0.6712 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "484/484 [==============================] - 0s 608us/step - loss: 0.9587 - accuracy: 0.0000e+00 - val_loss: 2.2256 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "484/484 [==============================] - 0s 621us/step - loss: 0.9889 - accuracy: 6.4599e-05 - val_loss: 0.8346 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "484/484 [==============================] - 0s 587us/step - loss: 1.0676 - accuracy: 6.4599e-05 - val_loss: 4.8567 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "484/484 [==============================] - 0s 621us/step - loss: 1.7957 - accuracy: 6.4599e-05 - val_loss: 0.6749 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "484/484 [==============================] - 0s 621us/step - loss: 0.9094 - accuracy: 1.2920e-04 - val_loss: 0.7422 - val_accuracy: 0.0000e+00\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, \n",
    "                input_shape=(8,),\n",
    "                activation='relu'))\n",
    "BatchNormalization(),\n",
    "model.add(Dense(128, activation='relu'))\n",
    "BatchNormalization(),\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, \n",
    "                           verbose=1, mode='auto')\n",
    "\n",
    "history= model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=None,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The results confirmed the number of Epoch to be 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9\n",
    "\n",
    "Adding `Dropout`.  Let's add a 5% dropout to the second layer, and a 20% dropout to the third layer and see if we end up stopping sooner or performing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "484/484 [==============================] - 0s 891us/step - loss: 525.1614 - accuracy: 0.0000e+00 - val_loss: 2.6767 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 3.1112 - accuracy: 6.4599e-05 - val_loss: 1.9804 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "484/484 [==============================] - 0s 793us/step - loss: 2.2041 - accuracy: 1.2920e-04 - val_loss: 1.3189 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "484/484 [==============================] - 0s 800us/step - loss: 1.9348 - accuracy: 0.0000e+00 - val_loss: 1.5152 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "484/484 [==============================] - 0s 790us/step - loss: 1.8201 - accuracy: 1.2920e-04 - val_loss: 1.3200 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "484/484 [==============================] - 0s 791us/step - loss: 1.7857 - accuracy: 0.0000e+00 - val_loss: 1.2776 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 1.6898 - accuracy: 0.0000e+00 - val_loss: 1.2236 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 1.6665 - accuracy: 0.0000e+00 - val_loss: 1.2117 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 1.5900 - accuracy: 0.0000e+00 - val_loss: 1.2460 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "484/484 [==============================] - 0s 786us/step - loss: 1.5243 - accuracy: 0.0000e+00 - val_loss: 1.1919 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "484/484 [==============================] - 0s 830us/step - loss: 1.4198 - accuracy: 1.2920e-04 - val_loss: 0.9308 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "484/484 [==============================] - 0s 821us/step - loss: 1.2350 - accuracy: 0.0000e+00 - val_loss: 0.9116 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 1.0733 - accuracy: 0.0000e+00 - val_loss: 0.8406 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "484/484 [==============================] - 0s 827us/step - loss: 0.9507 - accuracy: 0.0000e+00 - val_loss: 0.6976 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.8810 - accuracy: 0.0000e+00 - val_loss: 0.6445 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.8427 - accuracy: 0.0000e+00 - val_loss: 0.6702 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "484/484 [==============================] - 0s 827us/step - loss: 0.8000 - accuracy: 0.0000e+00 - val_loss: 0.5920 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7842 - accuracy: 0.0000e+00 - val_loss: 0.6180 - val_accuracy: 1.9380e-04\n",
      "Epoch 19/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.8903 - accuracy: 6.4599e-05 - val_loss: 0.7077 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "484/484 [==============================] - 0s 828us/step - loss: 0.8517 - accuracy: 0.0000e+00 - val_loss: 0.6394 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "484/484 [==============================] - 0s 814us/step - loss: 0.8157 - accuracy: 1.9380e-04 - val_loss: 0.6048 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.7653 - accuracy: 2.5840e-04 - val_loss: 0.6576 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "484/484 [==============================] - 0s 797us/step - loss: 0.7454 - accuracy: 0.0000e+00 - val_loss: 0.6376 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "484/484 [==============================] - 0s 791us/step - loss: 0.7522 - accuracy: 1.2920e-04 - val_loss: 0.7129 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 0.6082 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "484/484 [==============================] - 0s 827us/step - loss: 0.7497 - accuracy: 5.8140e-04 - val_loss: 0.6196 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "484/484 [==============================] - 0s 797us/step - loss: 0.7396 - accuracy: 1.9380e-04 - val_loss: 0.6078 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "484/484 [==============================] - 0s 831us/step - loss: 0.7325 - accuracy: 1.2920e-04 - val_loss: 0.6562 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "484/484 [==============================] - 0s 791us/step - loss: 0.7498 - accuracy: 6.4599e-05 - val_loss: 0.6809 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "484/484 [==============================] - 0s 827us/step - loss: 0.7241 - accuracy: 7.1059e-04 - val_loss: 0.6499 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.7338 - accuracy: 1.2920e-04 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "484/484 [==============================] - 0s 814us/step - loss: 0.7219 - accuracy: 2.5840e-04 - val_loss: 0.6713 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "484/484 [==============================] - 0s 802us/step - loss: 0.7156 - accuracy: 6.4599e-05 - val_loss: 0.5721 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "484/484 [==============================] - 0s 789us/step - loss: 0.7290 - accuracy: 0.0000e+00 - val_loss: 0.6068 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "484/484 [==============================] - 0s 827us/step - loss: 0.7178 - accuracy: 0.0000e+00 - val_loss: 0.6133 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7016 - accuracy: 0.0000e+00 - val_loss: 0.7239 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7115 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7058 - accuracy: 6.4599e-05 - val_loss: 0.5546 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.7124 - accuracy: 0.0000e+00 - val_loss: 0.5839 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "484/484 [==============================] - 0s 815us/step - loss: 0.6983 - accuracy: 0.0000e+00 - val_loss: 0.6183 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "484/484 [==============================] - 0s 799us/step - loss: 0.7259 - accuracy: 0.0000e+00 - val_loss: 0.5419 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "484/484 [==============================] - 0s 790us/step - loss: 0.7013 - accuracy: 0.0000e+00 - val_loss: 0.5752 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.9442 - accuracy: 6.4599e-05 - val_loss: 0.5683 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7202 - accuracy: 1.9380e-04 - val_loss: 0.5755 - val_accuracy: 3.8760e-04\n",
      "Epoch 45/100\n",
      "484/484 [==============================] - 0s 794us/step - loss: 0.7467 - accuracy: 3.8760e-04 - val_loss: 0.5931 - val_accuracy: 3.8760e-04\n",
      "Epoch 46/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7152 - accuracy: 6.4599e-05 - val_loss: 0.5486 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7083 - accuracy: 1.2920e-04 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "484/484 [==============================] - 0s 782us/step - loss: 0.7102 - accuracy: 5.8140e-04 - val_loss: 0.5477 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7057 - accuracy: 3.2300e-04 - val_loss: 0.5582 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7065 - accuracy: 6.4599e-05 - val_loss: 0.5346 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.6951 - accuracy: 0.0000e+00 - val_loss: 0.5537 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "484/484 [==============================] - 0s 794us/step - loss: 0.6938 - accuracy: 6.4599e-04 - val_loss: 0.5459 - val_accuracy: 9.6899e-04\n",
      "Epoch 53/100\n",
      "484/484 [==============================] - 0s 794us/step - loss: 0.6903 - accuracy: 6.4599e-05 - val_loss: 0.5714 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6775 - accuracy: 6.4599e-05 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "484/484 [==============================] - 0s 813us/step - loss: 0.6961 - accuracy: 3.8760e-04 - val_loss: 0.6281 - val_accuracy: 1.9380e-04\n",
      "Epoch 56/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.6888 - accuracy: 5.1680e-04 - val_loss: 0.5836 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6918 - accuracy: 5.8140e-04 - val_loss: 0.5347 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "484/484 [==============================] - 0s 798us/step - loss: 0.7005 - accuracy: 5.8140e-04 - val_loss: 0.5515 - val_accuracy: 1.9380e-04\n",
      "Epoch 59/100\n",
      "484/484 [==============================] - 0s 793us/step - loss: 0.6928 - accuracy: 1.2920e-04 - val_loss: 0.5208 - val_accuracy: 1.9380e-04\n",
      "Epoch 60/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.6936 - accuracy: 0.0000e+00 - val_loss: 0.5562 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "484/484 [==============================] - 0s 827us/step - loss: 0.6941 - accuracy: 0.0000e+00 - val_loss: 0.5350 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "484/484 [==============================] - 0s 794us/step - loss: 0.7127 - accuracy: 1.9380e-04 - val_loss: 0.6427 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "484/484 [==============================] - 0s 781us/step - loss: 0.6827 - accuracy: 0.0000e+00 - val_loss: 0.5510 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "484/484 [==============================] - 0s 826us/step - loss: 0.6878 - accuracy: 0.0000e+00 - val_loss: 0.5432 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "484/484 [==============================] - 0s 794us/step - loss: 0.6840 - accuracy: 0.0000e+00 - val_loss: 0.5212 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "484/484 [==============================] - 0s 791us/step - loss: 0.6841 - accuracy: 1.2920e-04 - val_loss: 0.5310 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "484/484 [==============================] - 0s 799us/step - loss: 0.6803 - accuracy: 0.0000e+00 - val_loss: 0.5331 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6784 - accuracy: 0.0000e+00 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6846 - accuracy: 0.0000e+00 - val_loss: 0.5589 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6625 - accuracy: 0.0000e+00 - val_loss: 0.5482 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "484/484 [==============================] - 0s 782us/step - loss: 0.6926 - accuracy: 0.0000e+00 - val_loss: 0.5823 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "484/484 [==============================] - 0s 827us/step - loss: 0.6824 - accuracy: 0.0000e+00 - val_loss: 0.5280 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.7022 - accuracy: 0.0000e+00 - val_loss: 0.5156 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "484/484 [==============================] - 0s 790us/step - loss: 0.6774 - accuracy: 0.0000e+00 - val_loss: 0.5214 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "484/484 [==============================] - 0s 791us/step - loss: 0.6846 - accuracy: 0.0000e+00 - val_loss: 0.5674 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6902 - accuracy: 0.0000e+00 - val_loss: 0.5322 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6849 - accuracy: 0.0000e+00 - val_loss: 0.5422 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.6847 - accuracy: 0.0000e+00 - val_loss: 0.5200 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "484/484 [==============================] - 0s 782us/step - loss: 0.6941 - accuracy: 0.0000e+00 - val_loss: 0.5181 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6648 - accuracy: 0.0030 - val_loss: 0.6861 - val_accuracy: 0.0021\n",
      "Epoch 81/100\n",
      "484/484 [==============================] - 0s 826us/step - loss: 0.6711 - accuracy: 0.0010 - val_loss: 0.5323 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "484/484 [==============================] - 0s 764us/step - loss: 0.6838 - accuracy: 0.0000e+00 - val_loss: 0.6396 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "484/484 [==============================] - 0s 763us/step - loss: 0.7085 - accuracy: 0.0000e+00 - val_loss: 0.5871 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6850 - accuracy: 0.0000e+00 - val_loss: 0.7408 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.6815 - accuracy: 0.0000e+00 - val_loss: 0.5677 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "484/484 [==============================] - 0s 782us/step - loss: 0.6823 - accuracy: 0.0000e+00 - val_loss: 0.5947 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "484/484 [==============================] - 0s 767us/step - loss: 0.6908 - accuracy: 0.0000e+00 - val_loss: 0.5365 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6775 - accuracy: 0.0000e+00 - val_loss: 0.5400 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "484/484 [==============================] - 0s 791us/step - loss: 0.6906 - accuracy: 0.0000e+00 - val_loss: 0.6458 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.6839 - accuracy: 6.4599e-05 - val_loss: 0.5515 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.6762 - accuracy: 0.0000e+00 - val_loss: 0.5357 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "484/484 [==============================] - 0s 796us/step - loss: 0.6974 - accuracy: 0.0000e+00 - val_loss: 0.5249 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6912 - accuracy: 0.0000e+00 - val_loss: 0.5290 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "484/484 [==============================] - 0s 791us/step - loss: 0.6829 - accuracy: 0.0000e+00 - val_loss: 0.5520 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "484/484 [==============================] - 0s 788us/step - loss: 0.6836 - accuracy: 0.0000e+00 - val_loss: 0.5214 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "484/484 [==============================] - 0s 827us/step - loss: 0.6728 - accuracy: 0.0000e+00 - val_loss: 0.5426 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6736 - accuracy: 0.0000e+00 - val_loss: 0.5574 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "484/484 [==============================] - 0s 795us/step - loss: 0.6704 - accuracy: 0.0000e+00 - val_loss: 0.5186 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "484/484 [==============================] - 0s 805us/step - loss: 0.6812 - accuracy: 0.0000e+00 - val_loss: 0.5349 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "484/484 [==============================] - 0s 787us/step - loss: 0.6798 - accuracy: 0.0000e+00 - val_loss: 0.5269 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "model_dp= Sequential()\n",
    "model_dp.add(Dense(64, \n",
    "                input_shape=(8,),\n",
    "                activation='relu'))\n",
    "model_dp.add(Dropout(0.05)),\n",
    "model_dp.add(Dense(128, activation='relu'))\n",
    "model_dp.add(Dropout(0.2)),\n",
    "model_dp.add(Dense(64))\n",
    "\n",
    "model_dp.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history_dp = model_dp.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9 - continued: RMSE vs. Baseline\n",
    "\n",
    "Compare the model aboves performance to that of the baseline model for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.00001    0.046754\n",
       "1.37500    0.005911\n",
       "1.62500    0.005669\n",
       "1.12500    0.004990\n",
       "1.87500    0.004506\n",
       "             ...   \n",
       "4.50800    0.000048\n",
       "0.32900    0.000048\n",
       "3.10100    0.000048\n",
       "3.26300    0.000048\n",
       "2.70400    0.000048\n",
       "Name: price, Length: 3842, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline of the model.\n",
    "cali_df['price'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real Values</th>\n",
       "      <th>Baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8158</th>\n",
       "      <td>2.285</td>\n",
       "      <td>2.070349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18368</th>\n",
       "      <td>2.799</td>\n",
       "      <td>2.070349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19197</th>\n",
       "      <td>1.830</td>\n",
       "      <td>2.070349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>4.658</td>\n",
       "      <td>2.070349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13073</th>\n",
       "      <td>1.500</td>\n",
       "      <td>2.070349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Real Values  Baseline\n",
       "8158         2.285  2.070349\n",
       "18368        2.799  2.070349\n",
       "19197        1.830  2.070349\n",
       "3746         4.658  2.070349\n",
       "13073        1.500  2.070349"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE of the model\n",
    "Baseline = pd.DataFrame({'Real Values':y_train, 'Baseline':y_train.mean()})\n",
    "Baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1551254785988831"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, Baseline['Baseline'])**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10: Regularization and Scaling\n",
    "\n",
    "Finally, we want to see if regularizing will improve the model.  Feed a model that is identical to the one above including dropout and include `l2` regularization in each of the dense layers of 0.01.  What is the RMSE of this model?  How does it compare to the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "model_l2= Sequential()\n",
    "model_l2.add(Dense(64, \n",
    "                input_shape=(8,),\n",
    "                activation='relu', kernel_regularizer=l2(0.01))),\n",
    "model_l2.add(Dense(128, activation='relu',  kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_l2.add(Dense(64))\n",
    "\n",
    "model_l2.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_l2 = model_l2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 11: Saving the Model\n",
    "\n",
    "Save the model as `cali_housing.h5`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2.save('cali_housing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
