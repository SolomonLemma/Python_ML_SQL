{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import liberary\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df= pd.read_csv('./401ksubs.csv')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first five data \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9275, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "- I think the following list may be helpful to additional variables to this dataset to have more information:\n",
    " - 1) the matching amount in each month \n",
    " - 2) how the person credit history is health based of credit score pay his agreement\n",
    " - 3) the personal asset status like car, house, shareholder or investment, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "- Race is more prone to unreasonable decisions. It may raise a lot of questions on the ethicality of the data and the process of making decisions based on this variable.. At the end, it will face serious unacceptance if someone will be qualified or not due to the race. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " **Answer:** \n",
    "- As it is clearly seen in the data and the dictionary inc used to calculate incsq which creates redundant. It may be good to drop either of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "\n",
    "- The variables `age`, `incsq` and `agesq` calculated with squaring the variable income and age. 401K is retirement insurance which needs qualified experts to explain what relation will have with age and also the income. When will be the relation may change as age closer to retirement time too. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "- It seems `age` and `inc` values are defined as an error since `age^2` and `inc^2`, respectively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "- a k-nearest neighbors model doesn't influence the feature, instead it depends on the distance of the features. On the contrary, linear regression, decision tree, random forest, adaboost, extremely randomized trees,and the XGBoost models have the influence of the features expressed with entropies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standared the data prior to modeling\n",
    "df_ds = pd.DataFrame(StandardScaler().fit_transform(df), columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "      <td>9.275000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.896473e-16</td>\n",
       "      <td>1.470761e-16</td>\n",
       "      <td>6.391294e-16</td>\n",
       "      <td>1.298751e-16</td>\n",
       "      <td>1.286063e-16</td>\n",
       "      <td>7.338604e-16</td>\n",
       "      <td>4.878996e-17</td>\n",
       "      <td>-2.069480e-15</td>\n",
       "      <td>-8.523879e-16</td>\n",
       "      <td>7.714703e-17</td>\n",
       "      <td>-2.350441e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "      <td>1.000054e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.031730e-01</td>\n",
       "      <td>-1.214123e+00</td>\n",
       "      <td>-1.300887e+00</td>\n",
       "      <td>-5.068978e-01</td>\n",
       "      <td>-1.561343e+00</td>\n",
       "      <td>-1.235500e+00</td>\n",
       "      <td>-8.151509e+00</td>\n",
       "      <td>-6.177763e-01</td>\n",
       "      <td>-5.840318e-01</td>\n",
       "      <td>-6.733840e-01</td>\n",
       "      <td>-1.304882e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.031730e-01</td>\n",
       "      <td>-7.304105e-01</td>\n",
       "      <td>-1.300887e+00</td>\n",
       "      <td>-5.068978e-01</td>\n",
       "      <td>-7.845661e-01</td>\n",
       "      <td>-5.800856e-01</td>\n",
       "      <td>-3.059968e-01</td>\n",
       "      <td>-6.177763e-01</td>\n",
       "      <td>-5.840318e-01</td>\n",
       "      <td>-5.504390e-01</td>\n",
       "      <td>-7.867935e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-8.031730e-01</td>\n",
       "      <td>-2.476946e-01</td>\n",
       "      <td>7.687061e-01</td>\n",
       "      <td>-5.068978e-01</td>\n",
       "      <td>-1.048859e-01</td>\n",
       "      <td>7.532845e-02</td>\n",
       "      <td>-2.669101e-01</td>\n",
       "      <td>-6.177763e-01</td>\n",
       "      <td>-5.840318e-01</td>\n",
       "      <td>-3.375534e-01</td>\n",
       "      <td>-2.162267e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.245062e+00</td>\n",
       "      <td>4.527167e-01</td>\n",
       "      <td>7.687061e-01</td>\n",
       "      <td>-5.068978e-01</td>\n",
       "      <td>6.718915e-01</td>\n",
       "      <td>7.307425e-01</td>\n",
       "      <td>-9.727507e-03</td>\n",
       "      <td>1.618709e+00</td>\n",
       "      <td>1.712236e+00</td>\n",
       "      <td>1.315537e-01</td>\n",
       "      <td>5.698381e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.245062e+00</td>\n",
       "      <td>6.633249e+00</td>\n",
       "      <td>7.687061e-01</td>\n",
       "      <td>1.972784e+00</td>\n",
       "      <td>2.225446e+00</td>\n",
       "      <td>6.629469e+00</td>\n",
       "      <td>2.372916e+01</td>\n",
       "      <td>1.618709e+00</td>\n",
       "      <td>1.712236e+00</td>\n",
       "      <td>1.249326e+01</td>\n",
       "      <td>2.570730e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              e401k           inc          marr          male           age  \\\n",
       "count  9.275000e+03  9.275000e+03  9.275000e+03  9.275000e+03  9.275000e+03   \n",
       "mean  -4.896473e-16  1.470761e-16  6.391294e-16  1.298751e-16  1.286063e-16   \n",
       "std    1.000054e+00  1.000054e+00  1.000054e+00  1.000054e+00  1.000054e+00   \n",
       "min   -8.031730e-01 -1.214123e+00 -1.300887e+00 -5.068978e-01 -1.561343e+00   \n",
       "25%   -8.031730e-01 -7.304105e-01 -1.300887e+00 -5.068978e-01 -7.845661e-01   \n",
       "50%   -8.031730e-01 -2.476946e-01  7.687061e-01 -5.068978e-01 -1.048859e-01   \n",
       "75%    1.245062e+00  4.527167e-01  7.687061e-01 -5.068978e-01  6.718915e-01   \n",
       "max    1.245062e+00  6.633249e+00  7.687061e-01  1.972784e+00  2.225446e+00   \n",
       "\n",
       "              fsize        nettfa         p401k          pira         incsq  \\\n",
       "count  9.275000e+03  9.275000e+03  9.275000e+03  9.275000e+03  9.275000e+03   \n",
       "mean   7.338604e-16  4.878996e-17 -2.069480e-15 -8.523879e-16  7.714703e-17   \n",
       "std    1.000054e+00  1.000054e+00  1.000054e+00  1.000054e+00  1.000054e+00   \n",
       "min   -1.235500e+00 -8.151509e+00 -6.177763e-01 -5.840318e-01 -6.733840e-01   \n",
       "25%   -5.800856e-01 -3.059968e-01 -6.177763e-01 -5.840318e-01 -5.504390e-01   \n",
       "50%    7.532845e-02 -2.669101e-01 -6.177763e-01 -5.840318e-01 -3.375534e-01   \n",
       "75%    7.307425e-01 -9.727507e-03  1.618709e+00  1.712236e+00  1.315537e-01   \n",
       "max    6.629469e+00  2.372916e+01  1.618709e+00  1.712236e+00  1.249326e+01   \n",
       "\n",
       "              agesq  \n",
       "count  9.275000e+03  \n",
       "mean  -2.350441e-16  \n",
       "std    1.000054e+00  \n",
       "min   -1.304882e+00  \n",
       "25%   -7.867935e-01  \n",
       "50%   -2.162267e-01  \n",
       "75%    5.698381e-01  \n",
       "max    2.570730e+00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Described indicate that the standared value equal to 1.\n",
    "df_ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the value\n",
    "X = df_ds.drop(columns = ['e401k', 'p401k', 'pira', 'inc', 'incsq'])\n",
    "y = df_ds['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_reg = LinearRegression()\n",
    "l_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestRegressor()\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "- It is a replacement method of a sample that can be used to simulate a different sample distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "- Decision trees are well known models which suffer from bias and variance. Whereas, Bagging is used to reduce the variance of a decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "- The random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree. The bagged decision trees of every feature are considered for splitting at a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "\n",
    "- In a random forest used to select the features randomly that are going to split. It helps to decreases the variance of the predictions by aggregating the different decision trees. This makes the random forest less variance than bagged decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE is 0.8288486281787126\n"
     ]
    }
   ],
   "source": [
    "# Calculate the RMSE for train in model l_reg\n",
    "trainRMSE = mean_squared_error(y_true = y_train, y_pred = l_reg.predict(X_train))\n",
    "trainRMSE_final = trainRMSE**0.5\n",
    "print(f'The training RMSE is {trainRMSE_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is\"  0.8771575950761487\n"
     ]
    }
   ],
   "source": [
    "# Calculate the RMSE for test in model l_reg\n",
    "testRMSE = mean_squared_error(y_true = y_test, y_pred = l_reg.predict(X_test))\n",
    "testRMSE_final = testRMSE **0.5\n",
    "print(f'The test RMSE is\"  {testRMSE_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE is 0.3158180481579853\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the RMSE to get the value\n",
    "trainRMSE = mean_squared_error(y_true = y_train, y_pred = random_forest.predict(X_train))\n",
    "trainRMSE_final = trainRMSE**0.5\n",
    "print(f'The training RMSE is {trainRMSE_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is\"  0.8510195042574749\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fuction to get the value\n",
    "# Calculate the RMSE for test in model l_reg\n",
    "testRMSE = mean_squared_error(y_true = y_test, y_pred = random_forest.predict(X_test))\n",
    "testRMSE_final = testRMSE **0.5\n",
    "print(f'The test RMSE is\"  {testRMSE_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note**\n",
    "- The results with random forest shows train and test have 50% different with its default evaluation of the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "The random forest shows much overfitting as compared with the linear regression taken as an example for this assignment. The reason is the RMSE of the train is much higher than the test value, which poorly generalizes the unseen 30% data .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "\n",
    "- For this problem the best choice highly depends on which model shows less difference value between train and test RSME. The smaller the difference is the less the overfit behavior and the best choice for the model. For instance linear regression shows less difference than random forest which have about 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "- a) Make feature engineering and feature treatment base of the distribution behaviour. For example, use log value for one side skewed to make a normal distribution.\n",
    "- b) Pick the models shows less overfitting nature using the  defaults hyperparameters evaluation.\n",
    "- c) List out the model best predicted accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "\n",
    "- Predict whether or not one is eligible for a 401k. The sentence may be biased by the final prediction someone will be eligible for the 401K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "- a logistic regression, knn, decision tree, Naive Bayes, random forest, extremely randomized trees and adaboost models can be used to predict whether or not one is eligible for a 401(k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning variable X and y, Idea taken from https://stackoverflow.com\n",
    "X = df_ds.drop(columns = ['e401k', 'p401k'])\n",
    "y = [1 if df_ds['e401k'][i] > 0 else 0 for i in range(df_ds.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evalution for classification\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evalution for classification\n",
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "- False positives means incorrectly predicted the person will be eligible for a 401k.\n",
    "- False negatives indicate the prediction made incorrectly the person not to be eligible for a 401k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "\n",
    "- The number of false positives predicted to be  as much as possible lower for voiding the ineligible one for 401K to be eligible. This caused financial risk to the IRA office and at large for the government of the country. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "\n",
    "- It would be great to minimize the specificity to avoid wrong classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "- Because the f1 value is evaluated with the denominator of false positive and false negative. This means the denominator values of either value increase, it will decrease the f1 value if the true positive value is constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_score of train using model knn: 0.6620194338825518\n"
     ]
    }
   ],
   "source": [
    "# f1_score for knn model for train\n",
    "train_f1_knn = f1_score(y_true = y_train, y_pred = knn.predict(X_train))\n",
    "print (f\"The f1_score of train using model knn: {train_f1_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_score of test using model knn: 0.4842105263157894\n"
     ]
    }
   ],
   "source": [
    "# f1_score for knn model for test\n",
    "test_f1_knn = f1_score(y_true = y_test, y_pred = knn.predict(X_test))\n",
    "print (f\"The f1_score of test using model knn: {test_f1_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_score of train using model knn: 0.5742279020234291\n"
     ]
    }
   ],
   "source": [
    "# f1_score for adaboost model for train\n",
    "train_f1_knn = f1_score(y_true = y_train, y_pred = adaboost.predict(X_train))\n",
    "print (f\"The f1_score of train using model knn: {train_f1_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_score of test using model knn: 0.5538150581101566\n"
     ]
    }
   ],
   "source": [
    "# f1_score for adaboost model for test\n",
    "test_f1_knn = f1_score(y_true = y_test, y_pred = adaboost.predict(X_test))\n",
    "print (f\"The f1_score of test using model knn: {test_f1_knn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "- Comparatively the knn shows higher value of f1 in train than test that causes overfitting as compared with adaboost which has 0.02 higher vale of train than the test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "- The primary thing I will look at is which model gives very close score values between train and test from all the models used in the problem. For this example, I will take adaboost performs very well with a closer score between train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "- May be getting recommendation for the best feature engineering to challenge the model for optimal performance.\n",
    "\n",
    "- May be looked critically for separately classifing the eligibility of the 401K since it is a very critical parameter before using the whole data to minimize the wrong filtration of ineligible to eligible catagory. \n",
    "\n",
    "- It may be better to work to find the optimal hyperparameters using GridSearch to have the best classification instead of using default values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
